{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@71f507a7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.1.1` \n",
    "import $ivy.`org.apache.spark::spark-mllib:3.1.1` \n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  AmmoniteSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.{HashingTF, OneHotEncoder, QuantileDiscretizer, StringIndexer, VectorAssembler}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.regression.{FMRegressionModel, FMRegressor}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.evaluation.RankingMetrics\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{avg, col, collect_list, dayofmonth, lit, month, struct, to_timestamp, udf, year}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.IntegerType\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mobject\u001b[39m \u001b[36mMovies\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.feature.{HashingTF, OneHotEncoder, QuantileDiscretizer, StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.{FMRegressionModel, FMRegressor}\n",
    "import org.apache.spark.mllib.evaluation.RankingMetrics\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions.{avg, col, collect_list, dayofmonth, lit, month, struct, to_timestamp, udf, year}\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "\n",
    "object Movies{\n",
    "\n",
    "    \n",
    "\n",
    "  val inputItemFeatureNames = \"\"\"item_id | movie_title | release_date | \n",
    "                                video release date | IMDb URL | unknown |  \n",
    "                                Action | Adventure | Animation | Children's  \n",
    "                                | Comedy | Crime | Documentary | Drama | Fantasy \n",
    "                                | Film-Noir | Horror | Musical | Mystery | Romance \n",
    "                                | Sci-Fi | Thriller | War | Western\"\"\".split(\"\\\\|\").map(_.trim)\n",
    "\n",
    "  val itemFeatureNames = inputItemFeatureNames\n",
    "      .filterNot(Array(\"release_date\", \"movie_title\",\n",
    "                       \"video release date\", \"IMDb URL\").contains(_)) ++ Seq(\"release_year\", \"release_month\", \"release_day\")\n",
    "  val items = readItems(inputItemFeatureNames).select(itemFeatureNames.map(col): _*).cache()\n",
    "\n",
    "  val userFeatureNames = \"user_id | age | gender | occupation | zip code\".split(\"\\\\|\").map(_.trim)\n",
    "  val users = readUsers(userFeatureNames).cache()\n",
    "    \n",
    "  val usersTransformedFeatureNames = Array(\"user_id\", \"ageDiscretized\", \"genderIndex\", \"occupationIndex\", \"zip codeIndex\")\n",
    "  val usersTransformed = getUserPipeline\n",
    "      .fit(users)\n",
    "      .transform(users)\n",
    "      .select(usersTransformedFeatureNames.map(col): _*)\n",
    "\n",
    "  val train_ratings = readTrainRatings()\n",
    "  val test_ratings = readTestRatings()\n",
    "  val (train_upsampled, test_upsampled) = addSomeZeroRatings(train_ratings, test_ratings)\n",
    "\n",
    "  val ratingsUpsampled = train_upsampled.union(test_upsampled)\n",
    "  \n",
    "\n",
    "  val data = ratingsUpsampled.join(items, \"item_id\").join(usersTransformed, \"user_id\")\n",
    "  val featureNames = Array(\"user_id\", \"item_id\") ++ itemFeatureNames.drop(1) ++ usersTransformedFeatureNames.drop(1)\n",
    "  val pipeline = getPipeline(featureNames).fit(data)\n",
    "\n",
    "  val train = pipeline.transform(train_upsampled.join(items, \"item_id\").join(usersTransformed, \"user_id\"))\n",
    "  val test = pipeline.transform(readTestRatings.join(items, \"item_id\").join(usersTransformed, \"user_id\"))\n",
    "\n",
    "  val (rankTrain, rankTest) = getTrainTestSets(train, test, ratingsUpsampled, usersTransformed, items, pipeline)\n",
    "  \n",
    "  def iter_factor_iter(factorizations: List[Int], iterations: List[Int]):  List[(Int, Int,\n",
    "                                                                                 Double, Double, Double, Double, Double)] = {\n",
    "     // param: принимает на вход массивы int для факторизации и итераций\n",
    "      // возвращает результат и время тренировки для каждой итерации в виде листа\n",
    "      var results: List[(Int, Int, Double, Double, Double, Double, Double)] = Nil\n",
    "      for (iteration <- iterations) {\n",
    "          for (factorization <- factorizations){\n",
    "             results :+= main(factorization, iteration, 20)}\n",
    "      }\n",
    "      return results\n",
    "\n",
    "  }\n",
    "  \n",
    "  def main(factorSize: Int = 30, maxIter: Int = 100, metricFor: Int = 20): (Int, Int, Double, Double, Double, Double, Double)  = {\n",
    "    \n",
    "    val t1 = System.nanoTime\n",
    "\n",
    "    val model = trainModel(train, factorSize, maxIter)\n",
    "      \n",
    "    val duration = (System.nanoTime - t1) / 1e9d\n",
    "    println(\"time of training \" + duration + \"\\n\")\n",
    "\n",
    "    println(\"Training set:\")\n",
    "\n",
    "    val (train_precision, train_recall) = rankingMetrics(rankTrain, train, model, spark, metricFor)\n",
    "\n",
    "    println(\"Testing set:\")\n",
    "    val (test_precision, test_recall) = rankingMetrics(rankTest, test, model, spark, metricFor)\n",
    "\n",
    "    return (factorSize, maxIter, duration, train_precision, train_recall, test_precision, test_recall)\n",
    "    \n",
    "  }\n",
    "\n",
    "\n",
    "  private def coldStartSplit(data: DataFrame, testSize: Double = 0.2, userColdStart: Boolean = true): (DataFrame, DataFrame) = {\n",
    "    val column = if (userColdStart) \"user_id\" else \"item_id\"\n",
    "\n",
    "    val unique = data.select(column).distinct()\n",
    "    val Array(trainUniqueIds, testUniqueIds) = unique.randomSplit(Array(1 - testSize, testSize))\n",
    "\n",
    "    val trainUnique = data.join(trainUniqueIds, column)\n",
    "    val testUnique = data.join(testUniqueIds, column)\n",
    "\n",
    "    (trainUnique, testUnique)\n",
    "  }\n",
    "\n",
    "  /** Adds negative samples. Approximately the same number of rows as in the ratings. */\n",
    "  private def addSomeZeroRatings(train_ratings: DataFrame, test_ratings: DataFrame, frac: Double = 0.08): (DataFrame, DataFrame) = {\n",
    "    val ratings = train_ratings.union(test_ratings)\n",
    "    val userIds = ratings.select(\"user_id\").distinct()\n",
    "    val itemIds = ratings.select(\"item_id\").distinct()\n",
    "    \n",
    "\n",
    "    val zeroRatings = userIds\n",
    "      .join(itemIds)\n",
    "      .join(ratings.select(\"user_id\", \"item_id\"), Seq(\"user_id\", \"item_id\"), \"left_anti\")\n",
    "      .sample(frac)\n",
    "      .withColumn(\"rating\", lit(0))\n",
    "    \n",
    "    val Array(train_zeros, test_zeros) = zeroRatings.randomSplit(Array(0.8, 0.2))\n",
    "    (train_ratings.union(train_zeros).cache(), test_ratings.union(test_zeros).cache())\n",
    "\n",
    "  }\n",
    "\n",
    "  /** Returns 2 DFs with cols: \"user_id\", \"item_id\", \"features\", \"rating\".\n",
    "   * Train set - each user from train DF with each item\n",
    "   * Test set - each user from test DF with each item\n",
    "   * */\n",
    "  private def getTrainTestSets(train: DataFrame,\n",
    "                               test: DataFrame,\n",
    "                               ratings: DataFrame,\n",
    "                               users: DataFrame,\n",
    "                               items: DataFrame,\n",
    "                               pipeline: PipelineModel): (DataFrame, DataFrame) = {\n",
    "      \n",
    "    val trainUserIds = train.select(\"user_id\").distinct()\n",
    "    val testUserIds = test.select(\"user_id\").distinct()\n",
    "\n",
    "    val itemIds = ratings.select(\"item_id\").distinct()\n",
    "\n",
    "    val trainUsersItems = trainUserIds.join(itemIds)\n",
    "    val testUsersItems = testUserIds\n",
    "      .join(itemIds)\n",
    "      .join(train, Seq(\"user_id\", \"item_id\"), \"left_anti\")\n",
    "\n",
    "    def getRankData(data: DataFrame): DataFrame = data\n",
    "      .join(users, \"user_id\")\n",
    "      .join(items, \"item_id\")\n",
    "      .join(ratings, Array(\"user_id\", \"item_id\"), \"left\")\n",
    "      .na.fill(0)\n",
    "\n",
    "    val trainRankData = getRankData(trainUsersItems)\n",
    "    val testRankData = getRankData(testUsersItems)\n",
    "\n",
    "    def transform(data: DataFrame): DataFrame =\n",
    "      pipeline.transform(data).select(\"user_id\", \"item_id\", \"features\", \"rating\")\n",
    "\n",
    "\n",
    "    (transform(trainRankData), transform(testRankData).cache())\n",
    "  }\n",
    "\n",
    "  private def getPipeline(featureNames: Array[String]): Pipeline = {\n",
    "    val oneHotEncoder = new OneHotEncoder()\n",
    "      .setInputCols(featureNames)\n",
    "      .setOutputCols(featureNames.map(_ + \"OHE\"))\n",
    "\n",
    "    val assembler = new VectorAssembler()\n",
    "      .setInputCols(featureNames.map(_ + \"OHE\"))\n",
    "      .setOutputCol(\"features\")\n",
    "\n",
    "    new Pipeline().setStages(Array(oneHotEncoder, assembler))\n",
    "  }\n",
    "\n",
    "  private def getUserPipeline = {\n",
    "    val columnNames = Array(\"gender\", \"occupation\", \"zip code\")\n",
    "    val stringIndexer = new StringIndexer()\n",
    "      .setInputCols(columnNames)\n",
    "      .setOutputCols(columnNames.map(_ + \"Index\"))\n",
    "\n",
    "    val discretizer = new QuantileDiscretizer()\n",
    "      .setInputCol(\"age\")\n",
    "      .setOutputCol(\"ageDiscretized\")\n",
    "      .setNumBuckets(10)\n",
    "\n",
    "    new Pipeline().setStages(Array(stringIndexer, discretizer))\n",
    "  }\n",
    "\n",
    "  private def rankingMetrics(data: DataFrame, \n",
    "                             truthData: DataFrame, \n",
    "                             model: FMRegressionModel,\n",
    "                             spark: SparkSession, metricFor: Int = 20): (Double, Double) = {\n",
    "    val t1 = System.nanoTime\n",
    "\n",
    "    import spark.implicits._\n",
    "\n",
    "    val predictions = model.transform(data).select(\"user_id\", \"item_id\", \"prediction\")\n",
    "\n",
    "    val recommendations = getRecommendations(predictions)\n",
    "    val truth = getUserWithItemsAsVector(truthData)\n",
    "\n",
    "    val evalData = recommendations.join(truth, \"user_id\")\n",
    "\n",
    "    val metrics = new RankingMetrics(evalData.select(\"prediction_items\", \"truth_items\").as[(Array[Int], Array[Int])].rdd)\n",
    "      \n",
    "    val precision =  metrics.precisionAt(metricFor)\n",
    "    val recall =  metrics.recallAt(metricFor)\n",
    "    val MAP = metrics.meanAveragePrecisionAt(metricFor)\n",
    "\n",
    "    println(s\"precisionAt($metricFor) \" + precision)\n",
    "    println(s\"recallAt($metricFor) \" + recall)\n",
    "    println(s\"meanAveragePrecisionAt($metricFor) \" + MAP)\n",
    "      \n",
    "    val duration = (System.nanoTime - t1) / 1e9d\n",
    "    println(\"time of predictions \" + duration + \"\\n\")\n",
    "    return (precision, recall)\n",
    "\n",
    "  }\n",
    "\n",
    "  /** DF: user_id, recommended_items as Vector.\n",
    "   *\n",
    "   * @param predictions DF: user_id, item_id, prediction_score */\n",
    "  private def getRecommendations(predictions: DataFrame) = {\n",
    "    val groupedPredictions = predictions.groupBy(\"user_id\")\n",
    "      .agg(collect_list(struct(\"prediction\", \"item_id\")) as \"rec_scores\")\n",
    "\n",
    "\n",
    "    val sortUdf = udf((l: Seq[Row]) => {\n",
    "      l.map({ case Row(p: Double, s: Integer) => (p, s) }).sortBy(_._1)(Ordering[Double].reverse).map(_._2) //.take(20)\n",
    "    })\n",
    "\n",
    "    groupedPredictions.select(col(\"user_id\"), sortUdf(col(\"rec_scores\")) as \"prediction_items\")\n",
    "  }\n",
    "\n",
    "  /** DF: user_id, Vector of items which user rated */\n",
    "  private def getUserWithItemsAsVector(truthData: DataFrame) = {\n",
    "    truthData\n",
    "      .where(col(\"rating\") =!= lit(0))\n",
    "      .groupBy(\"user_id\").agg(collect_list(\"item_id\") as \"truth_items\")\n",
    "  }\n",
    "\n",
    "  private def trainModel(dataAssembled: DataFrame, factorSize: Int = 30, maxIter: Int = 100): FMRegressionModel = {\n",
    "    println(s\"FactorSize = $factorSize, maxIter = $maxIter\")\n",
    "\n",
    "    val fm = new FMRegressor()\n",
    "      .setLabelCol(\"rating\")\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setFactorSize(factorSize)\n",
    "      .setMaxIter(maxIter)\n",
    "      .setRegParam(0.005)\n",
    "      .setStepSize(0.005)\n",
    "\n",
    "\n",
    "    fm.fit(dataAssembled)\n",
    "  }\n",
    "\n",
    " \n",
    "    private def readUsers(userFeatureNames: Array[String]) = {\n",
    "    spark.read\n",
    "      .option(\"header\", \"false\")\n",
    "      .option(\"delimiter\", \"|\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(\"./u.user\")\n",
    "      .toDF(userFeatureNames: _*)\n",
    "  }\n",
    "\n",
    "  private def readItems(itemFeatureNames: Array[String]) = {\n",
    "    spark.read\n",
    "      .option(\"header\", \"false\")\n",
    "      .option(\"delimiter\", \"|\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(\"./u.item\")\n",
    "      .toDF(itemFeatureNames: _*)\n",
    "      .na.fill(\"01-Jan-1995\", Seq(\"release_date\")) // Fill one missing with most frequent\n",
    "      .withColumn(\"release_date\", to_timestamp(col(\"release_date\"), \"d-MMM-yyyy\"))\n",
    "      .withColumn(\"release_year\", year(col(\"release_date\")))\n",
    "      .withColumn(\"release_month\", month(col(\"release_date\")))\n",
    "      .withColumn(\"release_day\", dayofmonth(col(\"release_date\")))\n",
    "  }\n",
    "\n",
    "  private def readRatings() = {\n",
    "    spark.read\n",
    "      .option(\"header\", \"false\")\n",
    "      .option(\"delimiter\", \"\\t\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(\"./u.data\")\n",
    "      .toDF(\"user_id\", \"item_id\", \"rating\", \"timestamp\")\n",
    "  }\n",
    "}\n",
    "    \n",
    "  private def readTrainRatings() = {\n",
    "      spark.read\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"delimiter\", \"\\t\")\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .csv(\"./train_ratings.csv\")\n",
    "          .toDF()\n",
    "  }\n",
    "\n",
    "  private def readTestRatings() = {\n",
    "      spark.read\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"delimiter\", \"\\t\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "          .csv(\"./test_ratings.csv\")\n",
    "          .toDF()\n",
    "  }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FactorSize = 10, maxIter = 50\n",
      "time of training 102.2901883\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.3097560975609755\n",
      "recallAt(20) 0.09924636029606941\n",
      "meanAveragePrecisionAt(20) 0.18715874805414864\n",
      "time of predictions 218.1152454\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.1513797634691196\n",
      "recallAt(20) 0.13180389343084778\n",
      "meanAveragePrecisionAt(20) 0.09270380613137973\n",
      "time of predictions 198.2461726\n",
      "\n",
      "FactorSize = 20, maxIter = 50\n",
      "time of training 125.0533345\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.31521739130434784\n",
      "recallAt(20) 0.10522695989101864\n",
      "meanAveragePrecisionAt(20) 0.19241865015101803\n",
      "time of predictions 207.0225407\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.15006570302233904\n",
      "recallAt(20) 0.13997009622779832\n",
      "meanAveragePrecisionAt(20) 0.09277988359273334\n",
      "time of predictions 9.0094201\n",
      "\n",
      "FactorSize = 30, maxIter = 50\n",
      "time of training 166.0967571\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.3586956521739132\n",
      "recallAt(20) 0.1281836452004867\n",
      "meanAveragePrecisionAt(20) 0.22035938138218672\n",
      "time of predictions 207.87276\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.17588699080157696\n",
      "recallAt(20) 0.17165916455568409\n",
      "meanAveragePrecisionAt(20) 0.11102407572042744\n",
      "time of predictions 13.5087853\n",
      "\n",
      "FactorSize = 40, maxIter = 50\n",
      "time of training 213.4030248\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.33080593849416756\n",
      "recallAt(20) 0.115419734691425\n",
      "meanAveragePrecisionAt(20) 0.19679438938574947\n",
      "time of predictions 212.0582396\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.16215505913272008\n",
      "recallAt(20) 0.15726607497923054\n",
      "meanAveragePrecisionAt(20) 0.09760363510772022\n",
      "time of predictions 14.9625015\n",
      "\n",
      "FactorSize = 50, maxIter = 50\n",
      "time of training 265.0497167\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.38393425238600215\n",
      "recallAt(20) 0.14459434548129163\n",
      "meanAveragePrecisionAt(20) 0.234713383178703\n",
      "time of predictions 219.5717393\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.18495400788436284\n",
      "recallAt(20) 0.1833680218532349\n",
      "meanAveragePrecisionAt(20) 0.11695969349137005\n",
      "time of predictions 14.6155481\n",
      "\n",
      "FactorSize = 100, maxIter = 50\n",
      "time of training 541.3699589\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.37470837751855796\n",
      "recallAt(20) 0.14309992655171286\n",
      "meanAveragePrecisionAt(20) 0.22342406703790763\n",
      "time of predictions 232.9234832\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.18120893561103818\n",
      "recallAt(20) 0.18120334336260868\n",
      "meanAveragePrecisionAt(20) 0.10965726731054311\n",
      "time of predictions 23.9712057\n",
      "\n",
      "FactorSize = 10, maxIter = 80\n",
      "time of training 123.8831459\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.3155355249204665\n",
      "recallAt(20) 0.10728781578720249\n",
      "meanAveragePrecisionAt(20) 0.18741436708737538\n",
      "time of predictions 218.0282889\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.1511826544021026\n",
      "recallAt(20) 0.14286041960016663\n",
      "meanAveragePrecisionAt(20) 0.09108608175929239\n",
      "time of predictions 13.4320464\n",
      "\n",
      "FactorSize = 20, maxIter = 80\n",
      "time of training 194.9265553\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.35837751855779415\n",
      "recallAt(20) 0.13432606225823085\n",
      "meanAveragePrecisionAt(20) 0.21376712482326232\n",
      "time of predictions 210.7683437\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.1767411300919842\n",
      "recallAt(20) 0.17562280958622198\n",
      "meanAveragePrecisionAt(20) 0.10799115366573163\n",
      "time of predictions 10.1929324\n",
      "\n",
      "FactorSize = 30, maxIter = 80\n",
      "time of training 257.3179001\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.38738069989395535\n",
      "recallAt(20) 0.14284883567268478\n",
      "meanAveragePrecisionAt(20) 0.23371159709671316\n",
      "time of predictions 237.7107359\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.19474375821287776\n",
      "recallAt(20) 0.19424280818133724\n",
      "meanAveragePrecisionAt(20) 0.12074046020458543\n",
      "time of predictions 11.5406777\n",
      "\n",
      "FactorSize = 40, maxIter = 80\n",
      "time of training 337.0812039\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.40450689289501596\n",
      "recallAt(20) 0.15417949878944337\n",
      "meanAveragePrecisionAt(20) 0.25143018477037615\n",
      "time of predictions 206.4009323\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.19908015768725357\n",
      "recallAt(20) 0.20369685407471025\n",
      "meanAveragePrecisionAt(20) 0.12868886248314326\n",
      "time of predictions 12.2700155\n",
      "\n",
      "FactorSize = 50, maxIter = 80\n",
      "time of training 408.2029832\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.4343584305408271\n",
      "recallAt(20) 0.16837723294868548\n",
      "meanAveragePrecisionAt(20) 0.27771369585913724\n",
      "time of predictions 225.6748525\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.21760840998685937\n",
      "recallAt(20) 0.230305741476136\n",
      "meanAveragePrecisionAt(20) 0.14420247128418082\n",
      "time of predictions 14.8968883\n",
      "\n",
      "FactorSize = 100, maxIter = 80\n",
      "time of training 825.5126443\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.4318663838812303\n",
      "recallAt(20) 0.16697582907283573\n",
      "meanAveragePrecisionAt(20) 0.2742179995959993\n",
      "time of predictions 215.6036047\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.21596583442838374\n",
      "recallAt(20) 0.2243957201832396\n",
      "meanAveragePrecisionAt(20) 0.14197767093023364\n",
      "time of predictions 27.3677592\n",
      "\n",
      "FactorSize = 10, maxIter = 100\n",
      "time of training 154.7668281\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.31744432661717925\n",
      "recallAt(20) 0.10875782127178127\n",
      "meanAveragePrecisionAt(20) 0.18577764342261088\n",
      "time of predictions 216.6984594\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.15749014454664917\n",
      "recallAt(20) 0.15128027963591753\n",
      "meanAveragePrecisionAt(20) 0.09238752622569879\n",
      "time of predictions 9.9114053\n",
      "\n",
      "FactorSize = 20, maxIter = 100\n",
      "time of training 235.9229044\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.3875397667020149\n",
      "recallAt(20) 0.14420359141848832\n",
      "meanAveragePrecisionAt(20) 0.23741352810424107\n",
      "time of predictions 199.4038551\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.19356110381077526\n",
      "recallAt(20) 0.1941985768815347\n",
      "meanAveragePrecisionAt(20) 0.12313717843768754\n",
      "time of predictions 9.0565018\n",
      "\n",
      "FactorSize = 30, maxIter = 100\n",
      "time of training 322.7969494\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.407423117709438\n",
      "recallAt(20) 0.15414459466993188\n",
      "meanAveragePrecisionAt(20) 0.2519212049874048\n",
      "time of predictions 237.4262866\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.20604467805519067\n",
      "recallAt(20) 0.21144752776031323\n",
      "meanAveragePrecisionAt(20) 0.13285113728067913\n",
      "time of predictions 12.373248\n",
      "\n",
      "FactorSize = 40, maxIter = 100\n",
      "time of training 408.9816429\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.43329798515376444\n",
      "recallAt(20) 0.16660259837440367\n",
      "meanAveragePrecisionAt(20) 0.27994228745642724\n",
      "time of predictions 204.7586348\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.22115637319316694\n",
      "recallAt(20) 0.23041416779809365\n",
      "meanAveragePrecisionAt(20) 0.14889762060979364\n",
      "time of predictions 13.7925017\n",
      "\n",
      "FactorSize = 50, maxIter = 100\n",
      "time of training 497.7576413\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.4660657476139981\n",
      "recallAt(20) 0.1803071233189591\n",
      "meanAveragePrecisionAt(20) 0.3124702204375956\n",
      "time of predictions 233.6261405\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.23856767411300925\n",
      "recallAt(20) 0.2495685510710579\n",
      "meanAveragePrecisionAt(20) 0.1671602368208445\n",
      "time of predictions 16.0307735\n",
      "\n",
      "FactorSize = 100, maxIter = 100\n",
      "time of training 1019.1212144\n",
      "\n",
      "Training set:\n",
      "precisionAt(20) 0.47211028632025454\n",
      "recallAt(20) 0.18323743193867376\n",
      "meanAveragePrecisionAt(20) 0.3185739875632993\n",
      "time of predictions 214.8335805\n",
      "\n",
      "Testing set:\n",
      "precisionAt(20) 0.23968462549277272\n",
      "recallAt(20) 0.24775337407702971\n",
      "meanAveragePrecisionAt(20) 0.17074298492669107\n",
      "time of predictions 20.2096187\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmy_results\u001b[39m: \u001b[32mList\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\n",
       "    \u001b[32m10\u001b[39m,\n",
       "    \u001b[32m50\u001b[39m,\n",
       "    \u001b[32m102.2901883\u001b[39m,\n",
       "    \u001b[32m0.3097560975609755\u001b[39m,\n",
       "    \u001b[32m0.09924636029606941\u001b[39m,\n",
       "    \u001b[32m0.1513797634691196\u001b[39m,\n",
       "    \u001b[32m0.13180389343084778\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m20\u001b[39m,\n",
       "    \u001b[32m50\u001b[39m,\n",
       "    \u001b[32m125.0533345\u001b[39m,\n",
       "    \u001b[32m0.31521739130434784\u001b[39m,\n",
       "    \u001b[32m0.10522695989101864\u001b[39m,\n",
       "    \u001b[32m0.15006570302233904\u001b[39m,\n",
       "    \u001b[32m0.13997009622779832\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m30\u001b[39m,\n",
       "    \u001b[32m50\u001b[39m,\n",
       "    \u001b[32m166.0967571\u001b[39m,\n",
       "    \u001b[32m0.3586956521739132\u001b[39m,\n",
       "    \u001b[32m0.1281836452004867\u001b[39m,\n",
       "    \u001b[32m0.17588699080157696\u001b[39m,\n",
       "    \u001b[32m0.17165916455568409\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m40\u001b[39m,\n",
       "    \u001b[32m50\u001b[39m,\n",
       "    \u001b[32m213.4030248\u001b[39m,\n",
       "    \u001b[32m0.33080593849416756\u001b[39m,\n",
       "    \u001b[32m0.115419734691425\u001b[39m,\n",
       "    \u001b[32m0.16215505913272008\u001b[39m,\n",
       "    \u001b[32m0.15726607497923054\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m50\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val my_results = Movies.iter_factor_iter(List(10, 20, 30, 40, 50, 100), List(50, 80, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: int, _2: int ... 5 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.createDataFrame(my_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.coalesce(1).write.csv(\"test_sparkFM.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
